{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable diffusion with Diffusers\n",
    "\n",
    "This is a notebook that demonstrates how to use the [Diffusers package](https://huggingface.co/docs/diffusers/index) from Huggingface to run stable diffusion. [Huggingface](https://huggingface.co) is a community-driven  platform that provides a comprehensive suite of open-source libraries and tools for machine learning. Think of it as a kind of \"github for machine learning\". The diffusers package provides a simple API and a number of pretrained models that allow to easily run and experiment with diffusion models. \n",
    "\n",
    "**NOTE** this notebook should work on other platform, but it has been only tested on Mac (M1). \n",
    "\n",
    "## Installation\n",
    "To use this notebook you will need to install diffusers with\n",
    "```\n",
    "pip install --upgrade diffusers\\[torch\\]\n",
    "pip install transformers\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running stable diffusion\n",
    "To run stable diffusion you will need to distinguish wether you are running this on a Mac M1/M2 or on Linux/Windows with a Nvidia GPU. \n",
    "On a M1/M2 Mac, diffusion will need a \"warmup\" phase to work properly (see [this link](https://huggingface.co/docs/diffusers/optimization/mps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this will load the pretrained model and download it the first time the cell is run (which might take a while). You can set the model version by modifying the `sd_version` variable. Read [this document](https://huggingface.co/docs/diffusers/en/stable_diffusion) for performance recommendations and tricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import DPMSolverMultistepScheduler\n",
    "\n",
    "sd_version = '2.1'\n",
    "\n",
    "if sd_version == '2.1':\n",
    "    model_key = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "elif sd_version == '2.0':\n",
    "    model_key = \"stabilityai/stable-diffusion-2-base\"\n",
    "elif sd_version == '1.5':\n",
    "    model_key = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_key, torch_dtype=torch.float16, use_safetensors=True)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config) # Faster scheduler\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "if device=='mps':\n",
    "    # Recommended if your computer has < 64 GB of RAM\n",
    "    pipe.enable_attention_slicing()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the image (note increasing `num_inference_steps` will improve quality but be slower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device).manual_seed(0) # Removing manual_seed will always generate different images \n",
    "\n",
    "prompt = \"A cubist painting of the Star Treck character Spock, high quality, trending on artstation\"\n",
    "image = pipe(prompt, guidance_scale=7.5, generator=generator, num_inference_steps=20).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(image))\n",
    "plt.title(prompt)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning stable diffusion with ControlNet\n",
    "\n",
    "[ControlNet](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Adding_Conditional_Control_to_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf) is a very recent and quite amazing advancement in image generation using stable diffusion. It allows conditioning the stable diffusion generation pipeline on an image input (similarly to pix2pix).  This system operates by integrating a smaller neural network with a pre-trained stable diffusion model. The weights of the stable diffusion model are frozen, and the combined model is trained to steer stable diffusion towards producing images consistent with the provided input conditions.\n",
    "\n",
    "The Huggingface diffusers API comes with ControlNet and a number of pre-trained models that can be used for an number of tasks such as guiding stable diffusion with edges, poses or depth maps (see https://huggingface.co/lllyasviel/ControlNet for available models).\n",
    "\n",
    "Here we will use the \"canny-to-image\" a ControlNet model, which guides stable diffusion with edge images. Let's start by using skimage to create edges from an input image, as we did with pix2pix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import io, feature, transform\n",
    "import PIL.Image as Image\n",
    "import cv2\n",
    "\n",
    "def apply_canny_skimage(img, sigma=1.5, size=512):\n",
    "    import cv2\n",
    "    from skimage import feature\n",
    "    invert = False\n",
    "    grayimg = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    edges = (feature.canny(grayimg, sigma=sigma)*255).astype(np.uint8)\n",
    "    if invert:\n",
    "        edges = cv2.bitwise_not(edges)\n",
    "    return cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "img = io.imread(\"images/spock.jpg\")\n",
    "edges =  apply_canny_skimage(img)\n",
    "\n",
    "# ControlNet expects a PIL Image as input\n",
    "edges_image = Image.fromarray(edges)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img)\n",
    "plt.subplot(122)\n",
    "plt.imshow(edges)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we setup the ControlNet model. Note that the current HuggingFace version of ControlNet requires Stable Diffusion 1.5 to run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
    "import torch\n",
    "import PIL.Image as Image\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-canny\", \n",
    "    torch_dtype=torch.float16)\n",
    "    \n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", # Controlnet is currently working only with SD 1.5\n",
    "    torch_dtype=torch.float16,\n",
    "    controlnet=controlnet, \n",
    "    safety_checker=None)\n",
    "print(\"converting to device\")\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "if device=='mps':\n",
    "    pipe.enable_attention_slicing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A sculpture made of clay on a white background\"\n",
    "\n",
    "generator = torch.Generator(device).manual_seed(0) # Removing manual_seed will always generate different images \n",
    "\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    image=edges_image,\n",
    "    generator=generator,\n",
    "    num_inference_steps=30,\n",
    ").images[0]\n",
    "\n",
    "plt.imshow(np.array(image))\n",
    "plt.title(prompt)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, we used edge detection to produce the ControlNet condition image. However, you can create interesting visual results also by procedurally generating an \"edge-like\" image, similarly to some examples we used with Pix2Pix.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py5canvas import canvas\n",
    "c = canvas.Canvas(512, 512)\n",
    "\n",
    "c.background(0)\n",
    "c.stroke_weight(1)\n",
    "c.stroke(255)\n",
    "c.no_fill()\n",
    "c.polygon(np.random.uniform(0, c.width, (20,2)))\n",
    "\n",
    "img = c.get_image()\n",
    "edges_image = Image.fromarray(img)\n",
    "edges_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A sculpture made of clay on a white background\"\n",
    "\n",
    "generator = torch.Generator(device).manual_seed(0) # Removing manual_seed will always generate different images \n",
    "\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    image=edges_image,\n",
    "    generator=generator,\n",
    "    num_inference_steps=30,\n",
    ").images[0]\n",
    "\n",
    "plt.imshow(np.array(image))\n",
    "plt.title(prompt)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
