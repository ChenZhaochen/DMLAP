{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable diffusion with Diffusers\n",
    "\n",
    "This is a notebook that demonstrates how to use the [Diffusers package](https://huggingface.co/docs/diffusers/index) from Huggingface to run stable diffusion. [Huggingface](https://huggingface.co) is a community-driven  platform that provides a comprehensive suite of open-source libraries and tools for machine learning. Think of it as a kind of \"github for machine learning\". The diffusers package provides a simple API and a number of pretrained models that allow to easily run and experiment with diffusion models. \n",
    "\n",
    "**NOTE** this notebook should work on other platform, but it has been only tested on Mac (M1). \n",
    "\n",
    "## Installation\n",
    "To use this notebook you will need to install diffusers with\n",
    "```\n",
    "pip install --upgrade diffusers\\[torch\\]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running stable diffusion\n",
    "To run stable diffusion you will need to distinguish wether you are running this on a Mac M1/M2 or on Linux/Windows with a Nvidia GPU. \n",
    "On a M1/M2 Mac, diffusion will need a \"warmup\" phase to work properly (see [this link](https://huggingface.co/docs/diffusers/optimization/mps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "if platform.processor() == 'arm':\n",
    "    print(\"Detected Mac M1/M2\")\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cuda'\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this will load the pretrained model and download it the first time the cell is run (which might take a while). You can set the model version by modifying the `sd_version` variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "sd_version = '2.1'\n",
    "\n",
    "if sd_version == '2.1':\n",
    "    model_key = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "elif sd_version == '2.0':\n",
    "    model_key = \"stabilityai/stable-diffusion-2-base\"\n",
    "elif sd_version == '1.5':\n",
    "    model_key = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "generator = StableDiffusionPipeline.from_pretrained(model_key)\n",
    "generator = generator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device=='mps':\n",
    "    # Recommended if your computer has < 64 GB of RAM\n",
    "    generator.enable_attention_slicing()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the image (note increasing `num_inference_steps` will improve quality but be slower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A cubist painting of the Star Treck character Spock, high quality, trending on artstation\"\n",
    "image = generator(prompt, guidance_scale=7.5, num_inference_steps=20).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(np.array(image))\n",
    "plt.title(prompt)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning stable diffusion with ControlNet\n",
    "\n",
    "[ControlNet](https://stablediffusionweb.com/ControlNet) is a very recent and quite amazing advancement in image generation using stable diffusion. It allows conditioning the stable diffusion generation pipeline on an image input (similarly to pix2pix).\n",
    "\n",
    "If you are running a Mac, you will need to update torch to the \"nightly\" version with\n",
    "```\n",
    "conda update pytorch torchvision -c pytorch-nightly\n",
    "```\n",
    "(see here: https://github.com/huggingface/diffusers/issues/2521)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use skimage to create edges from an input imgage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import io, feature, transform\n",
    "import PIL.Image as Image\n",
    "import cv2\n",
    "\n",
    "def apply_canny_skimage(img, sigma=1.5, size=512):\n",
    "    import cv2\n",
    "    from skimage import feature\n",
    "    invert = False\n",
    "    grayimg = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    edges = (feature.canny(grayimg, sigma=sigma)*255).astype(np.uint8)\n",
    "    if invert:\n",
    "        edges = cv2.bitwise_not(edges)\n",
    "    return cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "img = io.imread(\"images/spock.jpg\")\n",
    "edges =  apply_canny_skimage(img)\n",
    "\n",
    "# ControlNet expects a PIL Image as input\n",
    "edges_image = Image.fromarray(edges)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img)\n",
    "plt.subplot(122)\n",
    "plt.imshow(edges)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the ControlNet model. Somehow converting to device takes ages on Mac. Here we use the canny-to-image model, there are others available: \n",
    "[https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/controlnet](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/controlnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
    "import torch\n",
    "import PIL.Image as Image\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-canny\", \n",
    "    torch_dtype=torch.float16)\n",
    "    \n",
    "generator = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", # Controlnet is currently working only with SD 1.5\n",
    "    torch_dtype=torch.float16,\n",
    "    controlnet=controlnet, \n",
    "    safety_checker=None)\n",
    "print(\"converting to device\")\n",
    "generator = generator.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A sculpture made of clay on a white background\"\n",
    "\n",
    "if device=='mps':\n",
    "    generator.enable_attention_slicing()\n",
    "    #_ = generator(prompt, image=edges_image, num_inference_steps=1)\n",
    "elif device != 'cuda':\n",
    "    # this command loads the individual model components on GPU on-demand.\n",
    "    generator.enable_model_cpu_offload()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = generator(\n",
    "    prompt,\n",
    "    image=edges_image,\n",
    "    num_inference_steps=30,\n",
    ").images[0]\n",
    "\n",
    "plt.imshow(np.array(image))\n",
    "plt.title(prompt)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
